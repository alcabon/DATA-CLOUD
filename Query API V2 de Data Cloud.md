Excellent choix ! Nous quittons maintenant la th√©orie de l'architecture pour enfiler notre casquette de d√©veloppeur.

Bien que Salesforce pousse √©norm√©ment pour le "No-Code / Point-and-Click", la **Query API** (API de requ√™te) est l'arme secr√®te indispensable pour extraire et manipuler les donn√©es de mani√®re programmatique.

Voici comment fonctionne cette API, ce qu'elle permet de faire, et les pi√®ges √† √©viter.

---

### 1. Qu'est-ce que la Query API V2 de Data Cloud ?

C'est une API RESTful standard, mais avec une particularit√© puissante : **elle accepte des requ√™tes en ANSI SQL pur**.

Sous le capot, Data Cloud utilise un moteur de requ√™te distribu√© (bas√© sur la technologie open-source Trino/Presto) capable de lire les fameux fichiers Parquet et Iceberg dont nous parlions tout √† l'heure. La Query API est simplement la "porte d'entr√©e" pour parler √† ce moteur.

Vous pouvez interroger quasiment tout :

* Les **DLOs** (Donn√©es brutes)
* Les **DMOs** (Donn√©es harmonis√©es, ex: `Individual`, `SalesOrder`)
* Les **Profils Unifi√©s** (`UnifiedIndividual`)
* Les **Calculated Insights** (Pour r√©cup√©rer les scores d√©j√† calcul√©s)

### 2. Comment √ßa fonctionne techniquement ?

Le processus est tr√®s standard pour un d√©veloppeur habitu√© aux API web :

* **üîë L'Authentification :** Vous devez d'abord obtenir un jeton d'acc√®s (Access Token) via un flux OAuth 2.0 classique de Salesforce, avec les permissions sp√©cifiques √† Data Cloud.
* **üìç L'Endpoint :** Vous envoyez une requ√™te HTTP `POST` vers l'URL de votre instance Data Cloud. L'endpoint ressemble √† ceci :
`/api/v2/query`
* **üì¶ Le Payload (Corps de la requ√™te) :** C'est ici que vous √©crivez votre SQL, format√© en JSON.

### 3. Exemple concret d'une requ√™te

Imaginons que vous vouliez r√©cup√©rer le pr√©nom, le nom et l'e-mail des 10 profils unifi√©s ayant le plus d√©pens√© (en supposant que vous ayez un Calculated Insight appel√© `LifetimeValue`).

**Votre requ√™te (Payload envoy√© √† l'API) :**

```json
{
  "sql": "SELECT ssot__FirstName__c, ssot__LastName__c, ssot__Email__c FROM UnifiedIndividual__dlm ORDER BY LifetimeValue__c DESC LIMIT 10"
}

```

*(Note de l'expert : `ssot__` est le pr√©fixe standard des champs du mod√®le de donn√©es Salesforce, et `__dlm` signifie Data Lake Model, c'est l'extension des objets dans Data Cloud).*

**La r√©ponse de Data Cloud :**

```json
{
  "data": [
    {
      "ssot__FirstName__c": "Jean",
      "ssot__LastName__c": "Dupont",
      "ssot__Email__c": "jean.dupont@email.com"
    },
    {
      "ssot__FirstName__c": "Marie",
      "ssot__LastName__c": "Curie",
      "ssot__Email__c": "m.curie@email.com"
    }
  ],
  "startTime": "2024-10-25T10:00:00.000Z",
  "endTime": "2024-10-25T10:00:00.500Z",
  "rowCount": 2,
  "done": true
}

```

---

### 4. Pourquoi utiliser la Query API ? (Les cas d'usage)

Si on peut cr√©er des segments via l'interface, pourquoi s'emb√™ter √† coder en SQL ?

> **1. L'int√©gration avec des outils de BI externes :** Si votre entreprise utilise Tableau, PowerBI ou Looker, ces outils utilisent la Query API (via des connecteurs JDBC/ODBC) pour g√©n√©rer leurs tableaux de bord en interrogeant Data Cloud en temps r√©el.
> **2. L'extraction pour des applications sur mesure :** Vous d√©veloppez un portail client web personnalis√© et vous voulez afficher l'historique d'achat unifi√© du client connect√©. Votre site web va faire un appel √† la Query API avec une clause `WHERE Email = 'client@email.com'`.
> **3. Les jointures (JOIN) ultra-complexes :** L'interface graphique de segmentation a ses limites. Si vous devez faire des jointures complexes entre 5 tables diff√©rentes avec des sous-requ√™tes, l'API et le SQL pur sont votre seule solution.

### 5. ‚ö†Ô∏è Le point de vigilance de l'Architecte : La Pagination

C'est l'erreur num√©ro un des d√©veloppeurs juniors sur Data Cloud.
Nous sommes face √† un *Data Lake* contenant potentiellement des milliards de lignes. Si vous faites un simple `SELECT * FROM UnifiedIndividual` sans limite, vous feriez exploser le serveur.

L'API est con√ßue pour renvoyer un maximum de **100 000 lignes ou 8 Mo** par appel.
Si votre r√©sultat est plus volumineux, la valeur `"done": true` dans la r√©ponse JSON (vue dans l'exemple au-dessus) passera √† `"done": false`, et l'API vous fournira un `nextBatchId`. Vous devrez faire un nouvel appel API avec cet ID pour r√©cup√©rer la page suivante (le chunk), et ainsi de suite.

---

Maintenant que nous savons comment *lire* les donn√©es programmatiquement avec la Query API, il reste l'autre c√¥t√© du miroir.

Souhaitez-vous que nous abordions **l'Ingestion API** (pour voir comment un d√©veloppeur peut envoyer des flux de donn√©es en temps r√©el *vers* Data Cloud), ou pr√©f√©rez-vous d√©couvrir comment d√©clencher des automatisations avec les **Data Actions** ?

---

Oui, absolument ! Vous avez la bonne intuition. Bien que la technologie sous-jacente soit un *Data Lakehouse* (fichiers Parquet/Iceberg), **la repr√©sentation logique et visuelle des donn√©es se fait exactement comme dans une base de donn√©es relationnelle classique.**

Dans l'interface de Data Cloud (dans l'onglet *Data Model*), vous visualisez vos objets sous la forme d'un **Diagramme Entit√©-Association (ERD - Entity-Relationship Diagram)**.

Voici comment les concepts classiques de bases de donn√©es se traduisent dans l'univers Data Cloud.

### 1. La Mod√©lisation Relationnelle (PK, FK et Cardinalit√©)

Lorsque vous mappez vos donn√©es brutes (DLO) vers le mod√®le de donn√©es harmonis√© (DMO), vous devez d√©finir des relations strictes pour que le syst√®me puisse naviguer d'une table √† l'autre (notamment lors de la cr√©ation de segments).

* **Primary Key (Cl√© Primaire - PK) :** Chaque objet du mod√®le de donn√©es (DMO) *doit* avoir une cl√© primaire qui identifie de mani√®re unique chaque ligne. Par exemple, dans le DMO `Individual` (le client), ce sera souvent un champ nomm√© `IndividualId`.
* **Foreign Key (Cl√© √âtrang√®re - FK) :** Pour lier les tables entre elles, vous d√©finissez des relations en pointant un champ vers la Cl√© Primaire d'un autre objet. Par exemple, l'objet `ContactPointEmail` (qui stocke les adresses e-mail) aura un champ `PartyId` qui agit comme une cl√© √©trang√®re pointant vers le DMO `Individual`.
* **Cardinalit√© :** Lors de la cr√©ation de la relation dans l'interface graphique, vous devez sp√©cifier la nature de la jointure :
* **1:1 (One-to-One)**
* **1:N (One-to-Many) :** Le cas le plus fr√©quent (ex: Un `Individual` peut avoir plusieurs `SalesOrder`).
* **N:1 (Many-to-One)**



### 2. ‚ö†Ô∏è La Nuance de l'Architecte : L'Int√©grit√© R√©f√©rentielle

C'est ici qu'il faut faire tr√®s attention, car c'est une question classique de certification technique, et c'est ce qui diff√©rencie un *Data Lakehouse* d'une base de donn√©es Oracle ou SQL Server traditionnelle.

Dans une base de donn√©es classique, la relation PK/FK applique ce qu'on appelle une **contrainte d'int√©grit√© r√©f√©rentielle stricte**. Si vous essayez d'ins√©rer une commande (SalesOrder) pour un client (FK) qui n'existe pas dans la table Client, la base de donn√©es va bloquer l'insertion et g√©n√©rer une erreur.

**Dans Data Cloud, ce n'est pas le cas.**
Parce que nous sommes sur une architecture Big Data con√ßue pour ing√©rer des millions de lignes √† la seconde, Data Cloud effectue une **r√©solution √† la requ√™te (Schema-on-read)** et non √† l'√©criture.

* **Ce que √ßa signifie :** Si Data Cloud re√ßoit une ligne de commande avec un ID Client inconnu, *il va quand m√™me stocker la commande dans le Data Lake*. Il ne bloquera pas l'ingestion.
* **La cons√©quence :** C'est lors de la cr√©ation d'un Segment ou de l'ex√©cution d'une requ√™te SQL que la jointure se fera (ou √©chouera silencieusement pour cette ligne orpheline). La mod√©lisation PK/FK dans Data Cloud est donc une **couche s√©mantique** (pour dire au syst√®me *comment* lier les donn√©es quand on lui demande), mais ce n'est pas un *videur de bo√Æte de nuit* qui bloque les donn√©es √† l'entr√©e.

### 3. Le Mod√®le Standard : Le "Customer 360 Data Model"

Pour vous faciliter la vie, vous n'avez pas √† cr√©er toutes ces tables et cl√©s √©trang√®res de z√©ro.

Salesforce fournit un mod√®le canonique pr√©-construit (le *Customer 360 Data Model* ou *Cloud Information Model*). Il contient des dizaines de DMOs standards d√©j√† reli√©s entre eux avec les bonnes PK/FK.

Par exemple, le mod√®le structure la notion de "Personne" en s√©parant :

* `Individual` (La personne physique)
* `ContactPointEmail` (Ses adresses e-mail)
* `ContactPointPhone` (Ses num√©ros de t√©l√©phone)
* `ContactPointAddress` (Ses adresses postales)

L'avantage de respecter ce mod√®le canonique au lieu de cr√©er vos propres tables personnalis√©es, c'est que les moteurs d'Intelligence Artificielle (Einstein) et les r√®gles d'Identity Resolution savent exactement o√π chercher la donn√©e !

---

Maintenant que le fonctionnement de ce mod√®le relationnel est clair, nous avons deux voies techniques tr√®s int√©ressantes √† explorer.

Souhaitez-vous que nous abordions **les flux d'Ingestion API** (comment un d√©veloppeur pousse des donn√©es en temps r√©el dans ces tables) ou pr√©f√©rez-vous d√©couvrir **comment d√©clencher des automatisations avec les Data Actions** (par exemple, envoyer une alerte dans Slack ou Salesforce d√®s qu'un client VIP cr√©e un ticket de support) ?

---

Excellent choix ! Passer de la lecture (Query API) √† l'√©criture (Ingestion API) est l'√©tape ultime pour rendre votre Data Cloud v√©ritablement "vivant" et r√©actif.

Lorsqu'il s'agit de syst√®mes externes personnalis√©s (une application mobile sur mesure, un vieux syst√®me de caisse en magasin, ou un portail web sp√©cifique) pour lesquels il n'existe pas de connecteur natif, l'**Ingestion API** est la porte d'entr√©e incontournable.

Voici comment un d√©veloppeur orchestre ce flux de donn√©es, √©tape par √©tape, et les pi√®ges architecturaux √† √©viter.

---

### 1. Les deux "saveurs" de l'Ingestion API

En tant que d√©veloppeur, vous avez le choix entre deux m√©thodes d'ingestion, selon votre cas d'usage :

* **Streaming API (Flux en continu) :** C'est le mode temps r√©el (ou quasi temps r√©el). Vous l'utilisez pour envoyer de petits paquets de donn√©es (micro-batches) tr√®s fr√©quemment.
* *Cas d'usage :* Un utilisateur clique sur "Ajouter au panier" sur votre site web, ou met √† jour son profil sur votre application mobile. L'application envoie imm√©diatement le payload JSON.


* **Bulk API (Traitement par lots) :** C'est la m√©thode "lourde". Con√ßue pour ing√©rer des millions de lignes d'un coup.
* *Cas d'usage :* Le chargement initial historique de vos 10 derni√®res ann√©es de ventes, ou une synchronisation nocturne massive avec un vieil ERP. Les donn√©es sont envoy√©es sous forme de fichiers CSV via des "Jobs" (t√¢ches).



### 2. Le Pr√©requis : Le Contrat de Donn√©es (Schema)

Contrairement √† un Data Lake totalement anarchique o√π l'on pourrait jeter n'importe quel fichier JSON, Data Cloud exige un **contrat strict** avant d'accepter la moindre donn√©e.

1. L'administrateur Salesforce va dans l'interface de Data Cloud et cr√©e une nouvelle source de type **"Ingestion API"**.
2. Il doit y uploader un **Sch√©ma au format OpenAPI (YAML ou JSON)**. Ce fichier d√©crit exactement √† quoi va ressembler la donn√©e que le d√©veloppeur va envoyer (le nom des champs, s'il s'agit de texte, de nombres, de dates, et surtout, quel champ est la **Cl√© Primaire**).
3. Data Cloud cr√©e alors automatiquement l'objet r√©ceptacle brut (le fameux **DLO - Data Lake Object**) bas√© sur ce sch√©ma.

### 3. L'Appel API (Exemple avec le Streaming)

Une fois l'authentification (OAuth 2.0) g√©r√©e, le d√©veloppeur va cibler un endpoint sp√©cifique qui contient le nom de son connecteur et le nom de l'objet d√©fini dans le sch√©ma.

**Endpoint :**
`POST /api/v1/ingest/sources/MonSiteWeb/objects/VisiteurWeb`

**Le Payload (Corps de la requ√™te) :**
Le d√©veloppeur peut envoyer plusieurs enregistrements d'un coup (jusqu'√† 100 enregistrements ou 200 Ko par appel en Streaming).

```json
{
  "data": [
    {
      "IdSession": "sess-8899",
      "EmailClient": "jean.dupont@email.com",
      "Action": "AjoutPanier",
      "DateEvenement": "2026-02-13T20:40:00.000Z"
    },
    {
      "IdSession": "sess-9900",
      "EmailClient": "marie.c@email.com",
      "Action": "PageVue",
      "DateEvenement": "2026-02-13T20:41:05.000Z"
    }
  ]
}

```

La r√©ponse de l'API sera un code `202 Accepted`, signifiant que Data Cloud a bien re√ßu le paquet.

---

### 4. ‚ö†Ô∏è Le Secret de l'Architecte : L'Asynchronisme et l'Upsert

C'est ici que de nombreux d√©veloppeurs habitu√©s aux bases de donn√©es SQL classiques se font surprendre :

> **L'Upsert automatique (Mise √† jour ou Insertion) :**
> Vous n'avez pas besoin de faire de requ√™te `UPDATE`. L'Ingestion API fonctionne exclusivement en mode "Upsert" bas√© sur la Cl√© Primaire d√©finie dans votre sch√©ma.
> Si vous renvoyez un JSON avec un `IdSession` qui existe d√©j√†, Data Cloud √©crasera l'ancienne ligne. Si l'ID est nouveau, il cr√©era une nouvelle ligne.

> **C'est un processus Asynchrone :**
> Quand l'API renvoie le statut `202 Accepted`, cela veut dire que la donn√©e est *entr√©e* dans la file d'attente de la plateforme. Elle va atterrir dans le **DLO**. Ensuite, le moteur de Data Cloud va prendre le relais en arri√®re-plan pour ex√©cuter le mapping et transformer ce DLO en **DMO** (l'objet harmonis√©), puis potentiellement mettre √† jour le **Profil Unifi√©**.
> *Conclusion :* Si vous envoyez une donn√©e via l'Ingestion API, et que vous faites un `SELECT` via la Query API la milliseconde suivante, vous ne la verrez pas encore. Il y a un l√©ger d√©lai de traitement natif √† l'architecture Big Data.

---

Maintenant que vos d√©veloppeurs injectent des comportements clients en temps r√©el (comme l'ajout au panier) dans Data Cloud, c'est le moment id√©al pour exploiter cette r√©activit√© !

Souhaitez-vous que nous explorions **les Data Actions** pour voir comment d√©clencher automatiquement un √©v√©nement dans Salesforce CRM (ex: cr√©er une t√¢che pour relancer le client) d√®s que cette donn√©e d'abandon de panier arrive via l'API ?
